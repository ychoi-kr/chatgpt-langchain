{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ychoi-kr/chatgpt-langchain/blob/main/chapter3/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzrWQTLn70Ra"
      },
      "source": [
        "# ChatGPT를 API에서 이용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKSpjd8v8A9H"
      },
      "source": [
        "## 3-3 입력과 출력의 길이 제한 및 요금에 영향을 주는 '토큰'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6fSq7bc8DES"
      },
      "source": [
        "### Tokenizer와 tiktoken 소개"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9MPred-0se6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fcc0b36-8b45-49cd-a250-737b044c8ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.6.0) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.6.0) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WWdpn2pw8NGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edf01b3-5f77-4d57-e732-3768eee10e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "text = \"It’s easy to make something cool with LLMs, but very hard to make something production-ready with them.\"\n",
        "\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "tokens = encoding.encode(text)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vwIhYtWcshOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03e1c4e-b201-41a3-d51f-942d52696817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ],
      "source": [
        "text = \"LLM을 사용해서 멋져 보이는 것을 만들기는 쉽지만, 프로덕션 수준으로 만들어 내기는 매우 어렵다.\"\n",
        "\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "tokens = encoding.encode(text)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P56iAwzT8Xmq"
      },
      "source": [
        "## 3-4 Chat Completions API를 접하는 환경 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIZY8RE38cUC"
      },
      "source": [
        "### 구글 코랩 노트북 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r4TeCLj3Qe5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346ecdff-e5dd-4a86-bd19-1e297df87145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello World\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjYCIb2Q8fbe"
      },
      "source": [
        "### OpenAI API 키 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y4oSu5DqWgpG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 구글 코랩 보안 비밀에 API 키를 등록한 경우 다음 코드로 입력 가능\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "WNO0UI_cEwTR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0l8ptIe8iH5"
      },
      "source": [
        "## 3-5 Chat Completions API 사용해 보기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p9pLImp8kQ9"
      },
      "source": [
        "### OpenAI 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_R8jT7MNRC9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a5052d0-d5ad-49ef-c755-03431a45c680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==1.14.3 in /usr/local/lib/python3.10/dist-packages (1.14.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.14.3) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai==1.14.3) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.14.3) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.14.3) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.14.3) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai==1.14.3) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.14.3) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.14.3) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.14.3) (2.16.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==1.14.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTD-Xe9G8r6w"
      },
      "source": [
        "### Chat Completions API 호출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KHJUSC61Wg_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fd56d3-ba22-43bd-8441-3873f7bd5538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-99STGTd8g2zQZcasTHE1j13OeGzXW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello John! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1712041422, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_b28b39ffa8', usage=CompletionUsage(completion_tokens=10, prompt_tokens=23, total_tokens=33))\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello! I'm John.\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.model_dump_json(indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rqrmuHzQ39J",
        "outputId": "d971c3f9-c916-4e0d-b08c-dd41d9580d43"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-99STGTd8g2zQZcasTHE1j13OeGzXW\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"Hello John! How can I assist you today?\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": null\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1712041422,\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"system_fingerprint\": \"fp_b28b39ffa8\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 10,\n",
            "    \"prompt_tokens\": 23,\n",
            "    \"total_tokens\": 33\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2THywRt8t0F"
      },
      "source": [
        "### 대화 이력을 바탕으로 한 응답 얻기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Yk5Cc6rtWtub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1e39a7-866f-49e6-8851-998ba8ec1774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-99STHS5QX6wrcYokbrEAP4QUxfDjo\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"Yes, you mentioned your name is John. How can I help you further, John?\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": null\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1712041423,\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"system_fingerprint\": \"fp_b28b39ffa8\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 18,\n",
            "    \"prompt_tokens\": 47,\n",
            "    \"total_tokens\": 65\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello! I'm John.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"Hello John! How can I assist you today?\"},\n",
        "        {\"role\": \"user\", \"content\": \"Do you know my name?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKRqvj0u8xWL"
      },
      "source": [
        "### 응답을 스트리밍으로 받기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "V7J2Gq9PYPBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb9962a-98d5-4feb-b50f-63f367324a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hello\n",
            " John\n",
            "!\n",
            " How\n",
            " can\n",
            " I\n",
            " assist\n",
            " you\n",
            " today\n",
            "?\n"
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello! I'm John.\"}\n",
        "  ],\n",
        "  stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "  choice = chunk.choices[0]\n",
        "  if choice.finish_reason is None:\n",
        "    print(choice.delta.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqKaUhJ08zQ1"
      },
      "source": [
        "### （칼럼）Completions API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fD6Z-BZsCF3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20324d3c-5f2e-4809-8b91-f5bd3a1ba368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"cmpl-99STJsiIW6XCmRm75Dysa1YPjVyip\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"length\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" I am an AI designed to assist you with various tasks and provide helpful information.\\n\\n\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1712041425,\n",
            "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 16,\n",
            "    \"prompt_tokens\": 6,\n",
            "    \"total_tokens\": 22\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "response = client.completions.create(\n",
        "  model=\"gpt-3.5-turbo-instruct\",\n",
        "  prompt=\"Hello! I'm John.\"\n",
        ")\n",
        "\n",
        "print(response.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oKiq30SODvUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2639cc9b-6996-4a61-c23d-3399daeed0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"cmpl-99STJnTZXHlu17pWqJKaHCgemI8cP\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \"Yes, your name is John.\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1712041425,\n",
            "  \"model\": \"gpt-3.5-turbo-instruct\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 7,\n",
            "    \"prompt_tokens\": 28,\n",
            "    \"total_tokens\": 35\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"Human: Hello! I'm John.\n",
        "AI: Nice to meet you, John!\n",
        "Human: Do you know my name?\n",
        "AI: \"\"\"\n",
        "\n",
        "response = client.completions.create(\n",
        "  model=\"gpt-3.5-turbo-instruct\",\n",
        "  prompt=prompt\n",
        ")\n",
        "print(response.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlT300r83MS"
      },
      "source": [
        "## 3-6 Function calling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwG5cpzg85Vn"
      },
      "source": [
        "### Function calling의 샘플 코드\n",
        "\n",
        "[OpenAI 공식 문서](https://platform.openai.com/docs/guides/gpt/function-calling)를 바탕으로 일부 수정한 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K97Pbcw3bly8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def get_current_weather(location, unit=\"celsius\"):\n",
        "    weather_info = {\n",
        "        \"location\": location,\n",
        "        \"temperature\": \"25\",\n",
        "        \"unit\": \"celsius\",\n",
        "        \"forecast\": [\"sunny\", \"windy\"],\n",
        "    }\n",
        "    return json.dumps(weather_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PgIGMKtBgeGj"
      },
      "outputs": [],
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"description\": \"Get the current weather in a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g. Seoul\",\n",
        "                },\n",
        "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "            },\n",
        "            \"required\": [\"location\"],\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pO-VYo7tghih",
        "outputId": "f48f3ccc-6aba-4c59-d6f9-9d95a95c113f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-99STKrlHdZRUrKHsoM60Tv3Z6k0Gu\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"function_call\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": null,\n",
            "        \"role\": \"assistant\",\n",
            "        \"function_call\": {\n",
            "          \"arguments\": \"{\\\"location\\\":\\\"Seoul\\\"}\",\n",
            "          \"name\": \"get_current_weather\"\n",
            "        },\n",
            "        \"tool_calls\": null\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1712041426,\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"system_fingerprint\": \"fp_b28b39ffa8\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 16,\n",
            "    \"prompt_tokens\": 79,\n",
            "    \"total_tokens\": 95\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Seoul?\"}]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    functions=functions\n",
        ")\n",
        "\n",
        "print(response.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "c7LcVydmhlp0",
        "outputId": "6f5ec8c4-b8e2-4916-a410-4ade703cad7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"location\": \"Seoul\", \"temperature\": \"25\", \"unit\": \"celsius\", \"forecast\": [\"sunny\", \"windy\"]}\n"
          ]
        }
      ],
      "source": [
        "response_message = response.choices[0].message\n",
        "\n",
        "available_functions = {\n",
        "    \"get_current_weather\": get_current_weather,\n",
        "}\n",
        "function_name = response_message.function_call.name\n",
        "fuction_to_call = available_functions[function_name]\n",
        "function_args = json.loads(response_message.function_call.arguments)\n",
        "\n",
        "function_response = fuction_to_call(\n",
        "    location=function_args.get(\"location\"),\n",
        "    unit=function_args.get(\"unit\"),\n",
        ")\n",
        "\n",
        "print(function_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xe80yJNwjEJy"
      },
      "outputs": [],
      "source": [
        "messages.append(response_message)\n",
        "messages.append(\n",
        "    {\n",
        "        \"role\": \"function\",\n",
        "        \"name\": function_name,\n",
        "        \"content\": function_response,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XYXmxABim4b",
        "outputId": "1b87e705-4c18-4d90-a51e-53b2f4eb2ca7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'user', 'content': \"What's the weather like in Seoul?\"}, ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\"location\":\"Seoul\"}', name='get_current_weather'), tool_calls=None), {'role': 'function', 'name': 'get_current_weather', 'content': '{\"location\": \"Seoul\", \"temperature\": \"25\", \"unit\": \"celsius\", \"forecast\": [\"sunny\", \"windy\"]}'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Q9QBhpRtgtSW",
        "outputId": "9e893373-06f6-4baf-850d-bc33e14140d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-99STLIiqvY9XtkSI8igYDZGwq1J52\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"The weather in Seoul is currently 25°C and sunny with windy conditions.\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": null\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1712041427,\n",
            "  \"model\": \"gpt-3.5-turbo-0125\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"system_fingerprint\": \"fp_b28b39ffa8\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 15,\n",
            "    \"prompt_tokens\": 71,\n",
            "    \"total_tokens\": 86\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "second_response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "print(second_response.model_dump_json(indent=2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}